{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1acHYQauV9mw53tCSB_HStYvDHgu8er3V",
      "authorship_tag": "ABX9TyP0vHYqgYLRwuSYYJjHkTto",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SOURISHKUNDU/machine_learning/blob/main/ANN_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e9ztxQ5x33y-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36ce972b-f6f8-4f37-d07b-1fb03febd898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "def convert(A, Ttnp = True):\n",
        "    if Ttnp == True: return A.detach().cpu().numpy()\n",
        "    else: return torch.tensor(A).to(device)\n"
      ],
      "metadata": {
        "id": "7cdS73rc4cUG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root='/content/drive/MyDrive/Colab Notebooks'\n",
        "batch_size=1\n",
        "learning_rate = 0.001\n",
        "epochs = 300\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "def load(root):\n",
        "    input = pd.read_csv(os.path.join(root, 'input1.csv'))\n",
        "    target = pd.read_csv(os.path.join(root, 'target1.csv'))\n",
        "    input = np.array(input)\n",
        "    target = np.array(target)\n",
        "    X, Y = [], []\n",
        "    for i in range(input.shape[0]):\n",
        "        X.append(input[i])\n",
        "        Y.append(target[i])\n",
        "    \n",
        "    X = torch.tensor(X, dtype=torch.float32)\n",
        "    Y = torch.tensor(Y, dtype=torch.float32)\n",
        "    return X, Y   \n",
        "  "
      ],
      "metadata": {
        "id": "zj8mI92M4xvf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ANN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ANN, self).__init__()\n",
        "        self.ln = nn.LayerNorm(7)\n",
        "        self.layer1 = nn.Linear(7, 50)\n",
        "        self.ln1 = nn.LayerNorm(50)\n",
        "        self.layer = nn.Linear(50, 100)\n",
        "        self.lnm = nn.LayerNorm(100)\n",
        "        self.layer2 = nn.Linear(100, 50)\n",
        "        self.ln2 = nn.LayerNorm(50)\n",
        "        self.out = nn.Linear(50, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.ln(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.ln1(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.layer(x)\n",
        "        x = self.lnm(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        x = self.ln2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "pp3fQCOj6KJG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyFrame():\n",
        "    def __init__(self, ann, learning_rate, device, evalmode=False):\n",
        "        self.net = ann().to(device)\n",
        "        self.optimizer = torch.optim.Adam(params=list(self.net.parameters()), lr=learning_rate, weight_decay=0.0001)\n",
        "        self.loss = torch.nn.L1Loss()\n",
        "        self.lr = learning_rate\n",
        "\n",
        "    \n",
        "    def set_input(self, in_batch, target_batch=None):\n",
        "        self.input = in_batch\n",
        "        self.target = target_batch\n",
        "        \n",
        "    def optimize(self, backprop=True):\n",
        "        self.optimizer.zero_grad()\n",
        "        L = 0\n",
        "        predictions = self.net.forward(self.input)\n",
        "        L = self.loss(predictions, self.target)\n",
        "        if backprop:\n",
        "            L.backward()\n",
        "            self.optimizer.step()\n",
        "        return L.item(), predictions\n",
        "        \n",
        "    def save(self, path):\n",
        "        torch.save(self.Enc.state_dict(), path + '/' + 'ft_Enc' + '.pth')\n",
        "        \n",
        "        \n",
        "    def load(self, path):\n",
        "        self.Enc.load_state_dict(torch.load(path + '/' + 'ft_Enc' + '.pth', map_location=device))"
      ],
      "metadata": {
        "id": "eN3JnBoN6upG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = load(root)\n",
        "X_train, Y_train = X[0:45], Y[0:45]\n",
        "X_test, Y_test = X[45:], Y[45:]\n",
        "print(X.shape, Y.shape)\n",
        "print(X_train.shape, Y_train.shape)\n",
        "print(X_test.shape, Y_test.shape)\n",
        "\n",
        "#dividing my data into training and test states"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJ1QJ1nx65jV",
        "outputId": "cdea6fc5-3d49-4901-a20c-e11644edc69f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([55, 7]) torch.Size([55, 1])\n",
            "torch.Size([45, 7]) torch.Size([45, 1])\n",
            "torch.Size([10, 7]) torch.Size([10, 1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_length = X_train.shape[0]\n",
        "test_length = X_test.shape[0]\n",
        "\n",
        "solver = MyFrame(ANN, learning_rate, device)\n",
        "\n",
        "for epoch in range(500):\n",
        "    print('Epoch number: ', epoch)\n",
        "    #Training:\n",
        "    train_loss = 0\n",
        "    for i in range(train_length):\n",
        "        x = X_train[i]\n",
        "        y = Y_train[i]\n",
        "        solver.set_input(x,y)\n",
        "        l, y_h = solver.optimize()\n",
        "        train_loss += l\n",
        "    train_loss /= train_length\n",
        "    print('Train loss: ', train_loss)\n",
        "    \n",
        "    #Testing:\n",
        "    test_loss = 0\n",
        "    for i in range(test_length):\n",
        "        x = X_test[i]\n",
        "        y = Y_test[i]\n",
        "        solver.set_input(x,y)\n",
        "        l, y_h = solver.optimize(backprop=False)\n",
        "        test_loss += l\n",
        "    test_loss /= test_length\n",
        "    print('Test loss: ', test_loss)\n",
        "    print('----------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G89X6Lqa7N6c",
        "outputId": "ac0340e4-3169-4244-e728-44358e249052"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch number:  0\n",
            "Train loss:  45.2800916618771\n",
            "Test loss:  55.442961883544925\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  1\n",
            "Train loss:  44.0828795115153\n",
            "Test loss:  54.12618198394775\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  2\n",
            "Train loss:  42.840648322635225\n",
            "Test loss:  52.5770881652832\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  3\n",
            "Train loss:  41.39099111557007\n",
            "Test loss:  50.77935657501221\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  4\n",
            "Train loss:  39.7200442843967\n",
            "Test loss:  48.72665424346924\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  5\n",
            "Train loss:  37.918771192762584\n",
            "Test loss:  46.55285243988037\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  6\n",
            "Train loss:  36.033040576510956\n",
            "Test loss:  44.17076778411865\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  7\n",
            "Train loss:  33.93680843777127\n",
            "Test loss:  41.614112091064456\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  8\n",
            "Train loss:  31.60351062350803\n",
            "Test loss:  38.71865882873535\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  9\n",
            "Train loss:  28.867696873346965\n",
            "Test loss:  35.7289960861206\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  10\n",
            "Train loss:  26.184349213706124\n",
            "Test loss:  32.602338027954104\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  11\n",
            "Train loss:  23.53671598103311\n",
            "Test loss:  29.288538932800293\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  12\n",
            "Train loss:  21.098720017406677\n",
            "Test loss:  26.261795425415038\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  13\n",
            "Train loss:  19.232261988189485\n",
            "Test loss:  23.849466705322264\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  14\n",
            "Train loss:  17.576115794976552\n",
            "Test loss:  21.740410614013673\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  15\n",
            "Train loss:  16.32465274135272\n",
            "Test loss:  20.646452713012696\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  16\n",
            "Train loss:  15.32040157980389\n",
            "Test loss:  19.848692321777342\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  17\n",
            "Train loss:  14.796045589447022\n",
            "Test loss:  19.35758743286133\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  18\n",
            "Train loss:  15.04480934407976\n",
            "Test loss:  18.7190860748291\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  19\n",
            "Train loss:  15.172776440779367\n",
            "Test loss:  18.302230072021484\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  20\n",
            "Train loss:  14.917944208780925\n",
            "Test loss:  17.916775512695313\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  21\n",
            "Train loss:  14.461412694719103\n",
            "Test loss:  17.458633804321288\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  22\n",
            "Train loss:  13.814456702603234\n",
            "Test loss:  17.096543502807616\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  23\n",
            "Train loss:  13.565270254347059\n",
            "Test loss:  16.927556228637695\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  24\n",
            "Train loss:  13.329785153600906\n",
            "Test loss:  16.849394607543946\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  25\n",
            "Train loss:  13.406480373276604\n",
            "Test loss:  16.967270278930663\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  26\n",
            "Train loss:  13.338025122218662\n",
            "Test loss:  17.019082260131835\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  27\n",
            "Train loss:  12.87957744134797\n",
            "Test loss:  17.12037582397461\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  28\n",
            "Train loss:  13.32246464225981\n",
            "Test loss:  17.92099494934082\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  29\n",
            "Train loss:  12.401778037018246\n",
            "Test loss:  17.80481414794922\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  30\n",
            "Train loss:  12.305176529619429\n",
            "Test loss:  17.59464912414551\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  31\n",
            "Train loss:  12.176512720849779\n",
            "Test loss:  17.663972091674804\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  32\n",
            "Train loss:  11.703668814235263\n",
            "Test loss:  17.834737014770507\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  33\n",
            "Train loss:  12.141433164808486\n",
            "Test loss:  18.237219619750977\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  34\n",
            "Train loss:  11.422222501701778\n",
            "Test loss:  19.14507141113281\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  35\n",
            "Train loss:  11.407707108391655\n",
            "Test loss:  18.37369613647461\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  36\n",
            "Train loss:  11.136622228887346\n",
            "Test loss:  18.62176628112793\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  37\n",
            "Train loss:  10.928456579314338\n",
            "Test loss:  21.30471878051758\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  38\n",
            "Train loss:  10.722535965177748\n",
            "Test loss:  18.61613349914551\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  39\n",
            "Train loss:  10.426070612006717\n",
            "Test loss:  19.873084259033202\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  40\n",
            "Train loss:  10.243896900282966\n",
            "Test loss:  19.079809951782227\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  41\n",
            "Train loss:  10.541368002361722\n",
            "Test loss:  18.42687072753906\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  42\n",
            "Train loss:  10.644685859150357\n",
            "Test loss:  19.527229690551756\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  43\n",
            "Train loss:  10.150427663988538\n",
            "Test loss:  18.794998168945312\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  44\n",
            "Train loss:  10.108155479696062\n",
            "Test loss:  18.359900283813477\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  45\n",
            "Train loss:  10.595204810301462\n",
            "Test loss:  19.995225143432616\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  46\n",
            "Train loss:  9.931784074836306\n",
            "Test loss:  18.24245948791504\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  47\n",
            "Train loss:  10.344020137521955\n",
            "Test loss:  19.657919692993165\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  48\n",
            "Train loss:  9.82346087164349\n",
            "Test loss:  18.178880310058595\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  49\n",
            "Train loss:  10.047739483250512\n",
            "Test loss:  18.941067504882813\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  50\n",
            "Train loss:  10.009445198376973\n",
            "Test loss:  18.860190200805665\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  51\n",
            "Train loss:  9.452277037832472\n",
            "Test loss:  18.785118865966798\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  52\n",
            "Train loss:  9.609414471520317\n",
            "Test loss:  18.68149185180664\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  53\n",
            "Train loss:  9.532688695854612\n",
            "Test loss:  19.535251235961915\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  54\n",
            "Train loss:  9.240195055802664\n",
            "Test loss:  19.0260009765625\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  55\n",
            "Train loss:  9.594303259584638\n",
            "Test loss:  19.62720069885254\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  56\n",
            "Train loss:  9.162314494450888\n",
            "Test loss:  20.347003936767578\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  57\n",
            "Train loss:  9.611540814903048\n",
            "Test loss:  18.05066795349121\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  58\n",
            "Train loss:  9.405397548940446\n",
            "Test loss:  19.003686904907227\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  59\n",
            "Train loss:  9.202771694130368\n",
            "Test loss:  19.14583549499512\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  60\n",
            "Train loss:  9.734895606835684\n",
            "Test loss:  19.700136566162108\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  61\n",
            "Train loss:  9.140349120563931\n",
            "Test loss:  17.912835693359376\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  62\n",
            "Train loss:  8.905070790979597\n",
            "Test loss:  17.896435165405272\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  63\n",
            "Train loss:  9.249746965037453\n",
            "Test loss:  19.486893081665038\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  64\n",
            "Train loss:  8.637251100275252\n",
            "Test loss:  19.277484512329103\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  65\n",
            "Train loss:  8.497945880889892\n",
            "Test loss:  19.932834243774415\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  66\n",
            "Train loss:  10.004438541995155\n",
            "Test loss:  18.40883026123047\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  67\n",
            "Train loss:  8.999047338962555\n",
            "Test loss:  17.392189025878906\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  68\n",
            "Train loss:  8.456946623325347\n",
            "Test loss:  18.606791687011718\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  69\n",
            "Train loss:  8.217670089668697\n",
            "Test loss:  21.62294521331787\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  70\n",
            "Train loss:  8.552611749702029\n",
            "Test loss:  19.219111442565918\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  71\n",
            "Train loss:  9.152821211020152\n",
            "Test loss:  20.867556762695312\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  72\n",
            "Train loss:  9.514922862582736\n",
            "Test loss:  17.156881713867186\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  73\n",
            "Train loss:  8.646587842040592\n",
            "Test loss:  18.910480117797853\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  74\n",
            "Train loss:  8.199517691135407\n",
            "Test loss:  17.579176712036134\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  75\n",
            "Train loss:  8.349477820926243\n",
            "Test loss:  20.10707321166992\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  76\n",
            "Train loss:  9.665856317679088\n",
            "Test loss:  16.821280670166015\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  77\n",
            "Train loss:  8.347812742657132\n",
            "Test loss:  18.331494522094726\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  78\n",
            "Train loss:  8.10307972961002\n",
            "Test loss:  17.25142059326172\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  79\n",
            "Train loss:  8.45560328827964\n",
            "Test loss:  21.076065826416016\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  80\n",
            "Train loss:  9.023205949200523\n",
            "Test loss:  17.147785186767578\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  81\n",
            "Train loss:  8.29071117374632\n",
            "Test loss:  21.085119247436523\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  82\n",
            "Train loss:  7.994058448738522\n",
            "Test loss:  16.991625595092774\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  83\n",
            "Train loss:  8.085819702678256\n",
            "Test loss:  19.611041259765624\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  84\n",
            "Train loss:  8.365484878751968\n",
            "Test loss:  17.903319931030275\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  85\n",
            "Train loss:  8.103062295913697\n",
            "Test loss:  17.39795341491699\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  86\n",
            "Train loss:  7.817617187235091\n",
            "Test loss:  19.71694107055664\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  87\n",
            "Train loss:  7.577213793330722\n",
            "Test loss:  18.245629119873048\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  88\n",
            "Train loss:  7.841553350289662\n",
            "Test loss:  19.368583297729494\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  89\n",
            "Train loss:  8.146047989527384\n",
            "Test loss:  19.270555114746095\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  90\n",
            "Train loss:  7.730445887645086\n",
            "Test loss:  17.100727844238282\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  91\n",
            "Train loss:  8.051061324278514\n",
            "Test loss:  20.159862899780272\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  92\n",
            "Train loss:  8.809118600686391\n",
            "Test loss:  17.159299850463867\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  93\n",
            "Train loss:  8.067278120252821\n",
            "Test loss:  18.139320373535156\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  94\n",
            "Train loss:  7.884809532430437\n",
            "Test loss:  18.29440383911133\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  95\n",
            "Train loss:  7.818582197030385\n",
            "Test loss:  21.261143493652344\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  96\n",
            "Train loss:  8.403313771883647\n",
            "Test loss:  17.93842544555664\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  97\n",
            "Train loss:  7.3073790881368845\n",
            "Test loss:  19.50022964477539\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  98\n",
            "Train loss:  7.484404951996273\n",
            "Test loss:  18.88787384033203\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  99\n",
            "Train loss:  7.863679043451945\n",
            "Test loss:  18.744757080078124\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  100\n",
            "Train loss:  7.4316111445426944\n",
            "Test loss:  19.141911125183107\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  101\n",
            "Train loss:  7.2735282235675385\n",
            "Test loss:  18.703974151611327\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  102\n",
            "Train loss:  7.357304847240448\n",
            "Test loss:  19.08730697631836\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  103\n",
            "Train loss:  7.911245565944248\n",
            "Test loss:  17.627548217773438\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  104\n",
            "Train loss:  7.208542409208086\n",
            "Test loss:  18.456790924072266\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  105\n",
            "Train loss:  7.27926570839352\n",
            "Test loss:  17.918523406982423\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  106\n",
            "Train loss:  8.050760122140248\n",
            "Test loss:  17.703173446655274\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  107\n",
            "Train loss:  7.20860510137346\n",
            "Test loss:  19.184023284912108\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  108\n",
            "Train loss:  6.954834895663791\n",
            "Test loss:  18.701791763305664\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  109\n",
            "Train loss:  7.43811320066452\n",
            "Test loss:  15.732604598999023\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  110\n",
            "Train loss:  8.372787049081591\n",
            "Test loss:  18.275103759765624\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  111\n",
            "Train loss:  7.45524646308687\n",
            "Test loss:  19.21311683654785\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  112\n",
            "Train loss:  7.685844606161117\n",
            "Test loss:  17.54674873352051\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  113\n",
            "Train loss:  7.515984529919095\n",
            "Test loss:  18.089238357543945\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  114\n",
            "Train loss:  7.975881197717455\n",
            "Test loss:  19.04256401062012\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  115\n",
            "Train loss:  6.689101657602522\n",
            "Test loss:  20.006153106689453\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  116\n",
            "Train loss:  7.202514172924889\n",
            "Test loss:  18.16600112915039\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  117\n",
            "Train loss:  7.311417198181152\n",
            "Test loss:  17.87610855102539\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  118\n",
            "Train loss:  6.677081117365096\n",
            "Test loss:  18.60340118408203\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  119\n",
            "Train loss:  7.509985895951589\n",
            "Test loss:  17.998120498657226\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  120\n",
            "Train loss:  7.245306215021345\n",
            "Test loss:  18.080756378173827\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  121\n",
            "Train loss:  6.994324266910553\n",
            "Test loss:  15.53814926147461\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  122\n",
            "Train loss:  8.353848244084253\n",
            "Test loss:  17.709702682495116\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  123\n",
            "Train loss:  6.713617450661129\n",
            "Test loss:  18.18463134765625\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  124\n",
            "Train loss:  7.750684667958153\n",
            "Test loss:  16.394417190551756\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  125\n",
            "Train loss:  7.708929246001773\n",
            "Test loss:  18.643085861206053\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  126\n",
            "Train loss:  7.1324646327230665\n",
            "Test loss:  19.084893035888673\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  127\n",
            "Train loss:  6.7313392851087785\n",
            "Test loss:  18.658932495117188\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  128\n",
            "Train loss:  6.8789076871342125\n",
            "Test loss:  17.716349029541014\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  129\n",
            "Train loss:  6.953014381726583\n",
            "Test loss:  17.51187973022461\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  130\n",
            "Train loss:  7.667845384279887\n",
            "Test loss:  18.391944122314452\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  131\n",
            "Train loss:  6.404877546098497\n",
            "Test loss:  17.504504776000978\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  132\n",
            "Train loss:  6.949255816141764\n",
            "Test loss:  17.65957717895508\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  133\n",
            "Train loss:  6.586630144384173\n",
            "Test loss:  18.55979118347168\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  134\n",
            "Train loss:  7.00370010137558\n",
            "Test loss:  19.452380752563478\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  135\n",
            "Train loss:  6.539241884814368\n",
            "Test loss:  17.861270141601562\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  136\n",
            "Train loss:  6.769771938853793\n",
            "Test loss:  17.69204788208008\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  137\n",
            "Train loss:  6.508934067355262\n",
            "Test loss:  18.008999252319335\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  138\n",
            "Train loss:  6.542504237095515\n",
            "Test loss:  18.041227722167967\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  139\n",
            "Train loss:  7.370407256815168\n",
            "Test loss:  18.375407409667968\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  140\n",
            "Train loss:  7.097858276632097\n",
            "Test loss:  17.489452743530272\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  141\n",
            "Train loss:  7.048443310790592\n",
            "Test loss:  17.335120391845702\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  142\n",
            "Train loss:  7.5076321522394815\n",
            "Test loss:  17.98065528869629\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  143\n",
            "Train loss:  7.28639336294598\n",
            "Test loss:  16.547970962524413\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  144\n",
            "Train loss:  7.416258858309852\n",
            "Test loss:  17.292242813110352\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  145\n",
            "Train loss:  6.408413908216688\n",
            "Test loss:  17.32762680053711\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  146\n",
            "Train loss:  6.453333632151286\n",
            "Test loss:  16.839202880859375\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  147\n",
            "Train loss:  7.645436256461673\n",
            "Test loss:  18.680517196655273\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  148\n",
            "Train loss:  6.985119985209571\n",
            "Test loss:  17.888547897338867\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  149\n",
            "Train loss:  6.625145928064982\n",
            "Test loss:  18.319711685180664\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  150\n",
            "Train loss:  7.051273669136895\n",
            "Test loss:  18.688478469848633\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  151\n",
            "Train loss:  6.47787592543496\n",
            "Test loss:  17.97742118835449\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  152\n",
            "Train loss:  6.454769972960154\n",
            "Test loss:  17.097187805175782\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  153\n",
            "Train loss:  6.554805752966139\n",
            "Test loss:  17.968551254272462\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  154\n",
            "Train loss:  6.628064637713962\n",
            "Test loss:  17.239974975585938\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  155\n",
            "Train loss:  6.10883973704444\n",
            "Test loss:  17.984135818481445\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  156\n",
            "Train loss:  7.282009895642599\n",
            "Test loss:  17.312873458862306\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  157\n",
            "Train loss:  6.371080177360111\n",
            "Test loss:  17.92107582092285\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  158\n",
            "Train loss:  6.698066568374633\n",
            "Test loss:  17.12583808898926\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  159\n",
            "Train loss:  6.153727144665188\n",
            "Test loss:  19.339587783813478\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  160\n",
            "Train loss:  6.497734611564212\n",
            "Test loss:  17.645893096923828\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  161\n",
            "Train loss:  7.637623666392432\n",
            "Test loss:  17.349851989746092\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  162\n",
            "Train loss:  6.415587462319268\n",
            "Test loss:  16.826251602172853\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  163\n",
            "Train loss:  7.089338321155972\n",
            "Test loss:  17.88828773498535\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  164\n",
            "Train loss:  6.115981124507057\n",
            "Test loss:  17.712778091430664\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  165\n",
            "Train loss:  6.4861694680319895\n",
            "Test loss:  18.167564392089844\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  166\n",
            "Train loss:  6.533957076072693\n",
            "Test loss:  17.84568862915039\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  167\n",
            "Train loss:  6.026179136170281\n",
            "Test loss:  18.02521629333496\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  168\n",
            "Train loss:  7.083771234750747\n",
            "Test loss:  18.82993507385254\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  169\n",
            "Train loss:  6.2400403049257065\n",
            "Test loss:  18.202246475219727\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  170\n",
            "Train loss:  6.290308532449934\n",
            "Test loss:  18.036907958984376\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  171\n",
            "Train loss:  6.564515913857354\n",
            "Test loss:  17.91853561401367\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  172\n",
            "Train loss:  6.188578800360362\n",
            "Test loss:  18.506406402587892\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  173\n",
            "Train loss:  6.535560581419203\n",
            "Test loss:  16.797617721557618\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  174\n",
            "Train loss:  5.848153846793704\n",
            "Test loss:  18.797016906738282\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  175\n",
            "Train loss:  5.688275861740112\n",
            "Test loss:  17.660283660888673\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  176\n",
            "Train loss:  6.076740777492523\n",
            "Test loss:  17.92962989807129\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  177\n",
            "Train loss:  5.684649360179901\n",
            "Test loss:  17.299924087524413\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  178\n",
            "Train loss:  5.927339598867628\n",
            "Test loss:  17.84714546203613\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  179\n",
            "Train loss:  5.871853122446272\n",
            "Test loss:  19.00844383239746\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  180\n",
            "Train loss:  5.832416166199578\n",
            "Test loss:  18.666375732421876\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  181\n",
            "Train loss:  5.83367930650711\n",
            "Test loss:  19.063855743408205\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  182\n",
            "Train loss:  5.96353786389033\n",
            "Test loss:  17.88213233947754\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  183\n",
            "Train loss:  6.1147213631206085\n",
            "Test loss:  18.48826446533203\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  184\n",
            "Train loss:  6.480719642506705\n",
            "Test loss:  17.986543655395508\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  185\n",
            "Train loss:  6.228676856888665\n",
            "Test loss:  18.54846611022949\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  186\n",
            "Train loss:  5.994075743357341\n",
            "Test loss:  17.695793914794923\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  187\n",
            "Train loss:  5.990361228254106\n",
            "Test loss:  17.84121971130371\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  188\n",
            "Train loss:  6.271080482668347\n",
            "Test loss:  17.55272560119629\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  189\n",
            "Train loss:  6.246480309963227\n",
            "Test loss:  18.297658157348632\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  190\n",
            "Train loss:  6.179271252950032\n",
            "Test loss:  17.67300910949707\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  191\n",
            "Train loss:  5.606227150228288\n",
            "Test loss:  18.812868118286133\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  192\n",
            "Train loss:  6.48477861351437\n",
            "Test loss:  17.613191604614258\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  193\n",
            "Train loss:  5.757835650444031\n",
            "Test loss:  17.813107299804688\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  194\n",
            "Train loss:  5.909963018364376\n",
            "Test loss:  14.982132720947266\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  195\n",
            "Train loss:  6.6497925281524655\n",
            "Test loss:  18.499609756469727\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  196\n",
            "Train loss:  5.852858644061619\n",
            "Test loss:  17.22461853027344\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  197\n",
            "Train loss:  5.8797042939398025\n",
            "Test loss:  16.798601150512695\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  198\n",
            "Train loss:  5.8839862730767996\n",
            "Test loss:  18.353884124755858\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  199\n",
            "Train loss:  5.801085588667128\n",
            "Test loss:  17.555022048950196\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  200\n",
            "Train loss:  5.829457339975569\n",
            "Test loss:  17.71510887145996\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  201\n",
            "Train loss:  6.127205702993605\n",
            "Test loss:  19.68353691101074\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  202\n",
            "Train loss:  5.915444646941291\n",
            "Test loss:  18.025285720825195\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  203\n",
            "Train loss:  5.940097738636864\n",
            "Test loss:  17.684124374389647\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  204\n",
            "Train loss:  6.086193729771508\n",
            "Test loss:  19.485400390625\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  205\n",
            "Train loss:  6.308188839753469\n",
            "Test loss:  17.317209243774414\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  206\n",
            "Train loss:  5.63088258239958\n",
            "Test loss:  17.829345321655275\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  207\n",
            "Train loss:  5.244720146391127\n",
            "Test loss:  17.88575134277344\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  208\n",
            "Train loss:  6.020860964722104\n",
            "Test loss:  18.07624969482422\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  209\n",
            "Train loss:  5.687894430425432\n",
            "Test loss:  19.27795066833496\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  210\n",
            "Train loss:  5.6717046512497795\n",
            "Test loss:  17.474996185302736\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  211\n",
            "Train loss:  5.904606877432929\n",
            "Test loss:  19.415063095092773\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  212\n",
            "Train loss:  6.129244048065609\n",
            "Test loss:  17.243136596679687\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  213\n",
            "Train loss:  5.878493013646867\n",
            "Test loss:  17.578231048583984\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  214\n",
            "Train loss:  5.582967999246385\n",
            "Test loss:  18.381381225585937\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  215\n",
            "Train loss:  5.1384253051545885\n",
            "Test loss:  17.33718032836914\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  216\n",
            "Train loss:  5.392866628699832\n",
            "Test loss:  19.11513328552246\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  217\n",
            "Train loss:  5.756254751152462\n",
            "Test loss:  18.048646545410158\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  218\n",
            "Train loss:  5.663886896769205\n",
            "Test loss:  16.646654510498045\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  219\n",
            "Train loss:  5.523516970210605\n",
            "Test loss:  17.00424270629883\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  220\n",
            "Train loss:  5.293518955177731\n",
            "Test loss:  18.027180862426757\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  221\n",
            "Train loss:  5.193594735198551\n",
            "Test loss:  19.387392044067383\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  222\n",
            "Train loss:  5.198227236005995\n",
            "Test loss:  18.888486099243163\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  223\n",
            "Train loss:  5.375581759876675\n",
            "Test loss:  19.15202178955078\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  224\n",
            "Train loss:  5.526090640491909\n",
            "Test loss:  17.830537033081054\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  225\n",
            "Train loss:  5.386257353093889\n",
            "Test loss:  17.862882995605467\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  226\n",
            "Train loss:  5.406521635585361\n",
            "Test loss:  16.512124633789064\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  227\n",
            "Train loss:  5.323275532987383\n",
            "Test loss:  19.447124862670897\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  228\n",
            "Train loss:  6.039460473590427\n",
            "Test loss:  18.886013793945313\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  229\n",
            "Train loss:  5.538841329680549\n",
            "Test loss:  18.121845626831053\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  230\n",
            "Train loss:  5.236983582708571\n",
            "Test loss:  18.631596755981445\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  231\n",
            "Train loss:  5.405451314979129\n",
            "Test loss:  19.324545288085936\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  232\n",
            "Train loss:  5.141916147867838\n",
            "Test loss:  19.18824462890625\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  233\n",
            "Train loss:  5.195616841316223\n",
            "Test loss:  18.659844970703126\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  234\n",
            "Train loss:  5.239888722366757\n",
            "Test loss:  21.353709411621093\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  235\n",
            "Train loss:  4.935427767700619\n",
            "Test loss:  17.07231788635254\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  236\n",
            "Train loss:  5.292976127730475\n",
            "Test loss:  20.08584213256836\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  237\n",
            "Train loss:  5.122735559940338\n",
            "Test loss:  19.15034065246582\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  238\n",
            "Train loss:  5.0069330321417915\n",
            "Test loss:  19.353916549682616\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  239\n",
            "Train loss:  4.630023711257511\n",
            "Test loss:  19.422607803344725\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  240\n",
            "Train loss:  5.1964208139313595\n",
            "Test loss:  17.781586837768554\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  241\n",
            "Train loss:  5.51304810974333\n",
            "Test loss:  18.870975494384766\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  242\n",
            "Train loss:  6.350481145911747\n",
            "Test loss:  18.437635803222655\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  243\n",
            "Train loss:  5.194098708364699\n",
            "Test loss:  16.806415939331053\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  244\n",
            "Train loss:  5.432855741182963\n",
            "Test loss:  22.37625923156738\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  245\n",
            "Train loss:  5.014120342996385\n",
            "Test loss:  19.178437805175783\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  246\n",
            "Train loss:  4.74873312579261\n",
            "Test loss:  17.531063842773438\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  247\n",
            "Train loss:  5.57458008395301\n",
            "Test loss:  18.77301788330078\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  248\n",
            "Train loss:  4.766587796476152\n",
            "Test loss:  20.083458709716798\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  249\n",
            "Train loss:  5.602457911438412\n",
            "Test loss:  19.220892715454102\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  250\n",
            "Train loss:  5.164996598826514\n",
            "Test loss:  18.723382186889648\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  251\n",
            "Train loss:  5.836530001958211\n",
            "Test loss:  19.439377212524413\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  252\n",
            "Train loss:  4.9067753513654075\n",
            "Test loss:  17.805967712402342\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  253\n",
            "Train loss:  4.963735564549764\n",
            "Test loss:  19.428332138061524\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  254\n",
            "Train loss:  5.426336128181881\n",
            "Test loss:  19.58514747619629\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  255\n",
            "Train loss:  4.77131652434667\n",
            "Test loss:  18.71716728210449\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  256\n",
            "Train loss:  4.668282650576698\n",
            "Test loss:  18.363647079467775\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  257\n",
            "Train loss:  4.594574244817098\n",
            "Test loss:  17.735791015625\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  258\n",
            "Train loss:  5.09891156885359\n",
            "Test loss:  22.257533645629884\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  259\n",
            "Train loss:  4.9887109253141615\n",
            "Test loss:  20.72742118835449\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  260\n",
            "Train loss:  4.5844077348709105\n",
            "Test loss:  19.094490814208985\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  261\n",
            "Train loss:  4.452216353681353\n",
            "Test loss:  19.148843002319335\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  262\n",
            "Train loss:  4.630512200461494\n",
            "Test loss:  19.5875301361084\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  263\n",
            "Train loss:  5.755633427037133\n",
            "Test loss:  17.977279281616212\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  264\n",
            "Train loss:  4.6532043827904594\n",
            "Test loss:  19.41087074279785\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  265\n",
            "Train loss:  4.982764646742079\n",
            "Test loss:  22.84342441558838\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  266\n",
            "Train loss:  4.903777788745033\n",
            "Test loss:  17.583800506591796\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  267\n",
            "Train loss:  5.220455480946435\n",
            "Test loss:  22.277145767211913\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  268\n",
            "Train loss:  4.341223288906945\n",
            "Test loss:  20.23983039855957\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  269\n",
            "Train loss:  4.302686901887258\n",
            "Test loss:  19.526704788208008\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  270\n",
            "Train loss:  4.367460449536641\n",
            "Test loss:  19.838916397094728\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  271\n",
            "Train loss:  4.437710653411018\n",
            "Test loss:  21.31977653503418\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  272\n",
            "Train loss:  4.644492383797964\n",
            "Test loss:  20.036605072021484\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  273\n",
            "Train loss:  4.303735347588857\n",
            "Test loss:  17.795704650878907\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  274\n",
            "Train loss:  5.179929490884145\n",
            "Test loss:  19.034290313720703\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  275\n",
            "Train loss:  4.374986990292867\n",
            "Test loss:  16.757534790039063\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  276\n",
            "Train loss:  4.751444001992543\n",
            "Test loss:  20.445356369018555\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  277\n",
            "Train loss:  4.112281766202715\n",
            "Test loss:  21.201342582702637\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  278\n",
            "Train loss:  4.652342783080207\n",
            "Test loss:  17.53654899597168\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  279\n",
            "Train loss:  5.0411612311999\n",
            "Test loss:  18.79915428161621\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  280\n",
            "Train loss:  4.089434583981832\n",
            "Test loss:  16.234935760498047\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  281\n",
            "Train loss:  5.383960792753432\n",
            "Test loss:  16.513840103149413\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  282\n",
            "Train loss:  4.488559215598636\n",
            "Test loss:  16.601625061035158\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  283\n",
            "Train loss:  4.598086966408624\n",
            "Test loss:  18.920365142822266\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  284\n",
            "Train loss:  3.800260885556539\n",
            "Test loss:  20.91664848327637\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  285\n",
            "Train loss:  4.432327148649428\n",
            "Test loss:  19.83499412536621\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  286\n",
            "Train loss:  4.139341164959801\n",
            "Test loss:  21.739640045166016\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  287\n",
            "Train loss:  4.57602537340588\n",
            "Test loss:  13.666184234619141\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  288\n",
            "Train loss:  4.967950704362657\n",
            "Test loss:  19.803094482421876\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  289\n",
            "Train loss:  4.184355029794905\n",
            "Test loss:  20.15076599121094\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  290\n",
            "Train loss:  4.189897582266066\n",
            "Test loss:  16.5470458984375\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  291\n",
            "Train loss:  4.495590335792965\n",
            "Test loss:  18.645591735839844\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  292\n",
            "Train loss:  3.764332577917311\n",
            "Test loss:  18.934264373779296\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  293\n",
            "Train loss:  4.309771919250489\n",
            "Test loss:  17.001471710205077\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  294\n",
            "Train loss:  4.616454988055759\n",
            "Test loss:  20.726132965087892\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  295\n",
            "Train loss:  3.902758475144704\n",
            "Test loss:  22.00161247253418\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  296\n",
            "Train loss:  3.726564027203454\n",
            "Test loss:  18.168675231933594\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  297\n",
            "Train loss:  4.543115161524879\n",
            "Test loss:  16.292007827758788\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  298\n",
            "Train loss:  4.146052575111389\n",
            "Test loss:  20.468320465087892\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  299\n",
            "Train loss:  3.8049093034532335\n",
            "Test loss:  22.17085247039795\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  300\n",
            "Train loss:  4.204150152206421\n",
            "Test loss:  18.302752685546874\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  301\n",
            "Train loss:  4.5021631373299495\n",
            "Test loss:  24.11935348510742\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  302\n",
            "Train loss:  4.100099082787832\n",
            "Test loss:  25.084330749511718\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  303\n",
            "Train loss:  3.7017127752304075\n",
            "Test loss:  19.59002265930176\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  304\n",
            "Train loss:  3.7741956538624235\n",
            "Test loss:  14.753381729125977\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  305\n",
            "Train loss:  4.325467336177826\n",
            "Test loss:  16.681721878051757\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  306\n",
            "Train loss:  4.854951781696744\n",
            "Test loss:  21.540869140625\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  307\n",
            "Train loss:  3.9421679589483474\n",
            "Test loss:  24.492134284973144\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  308\n",
            "Train loss:  3.8702293806605867\n",
            "Test loss:  17.279359817504883\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  309\n",
            "Train loss:  4.164374840259552\n",
            "Test loss:  17.870947647094727\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  310\n",
            "Train loss:  4.219543622599708\n",
            "Test loss:  21.536221313476563\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  311\n",
            "Train loss:  4.3067795541551375\n",
            "Test loss:  16.378933715820313\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  312\n",
            "Train loss:  5.110396405061086\n",
            "Test loss:  19.908173751831054\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  313\n",
            "Train loss:  4.3366479661729604\n",
            "Test loss:  19.705278396606445\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  314\n",
            "Train loss:  3.8071576171451147\n",
            "Test loss:  19.893817901611328\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  315\n",
            "Train loss:  3.9473914000723096\n",
            "Test loss:  21.86939468383789\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  316\n",
            "Train loss:  3.8232344071070354\n",
            "Test loss:  25.523908805847167\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  317\n",
            "Train loss:  4.293580589029524\n",
            "Test loss:  21.290350341796874\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  318\n",
            "Train loss:  3.5555364343855116\n",
            "Test loss:  19.13737335205078\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  319\n",
            "Train loss:  4.42027637163798\n",
            "Test loss:  18.141373062133788\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  320\n",
            "Train loss:  3.9438389394018385\n",
            "Test loss:  19.9124116897583\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  321\n",
            "Train loss:  3.7779930339919194\n",
            "Test loss:  16.194883346557617\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  322\n",
            "Train loss:  4.145116032494439\n",
            "Test loss:  23.26612548828125\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  323\n",
            "Train loss:  4.207986917760637\n",
            "Test loss:  18.785337448120117\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  324\n",
            "Train loss:  3.783135928048028\n",
            "Test loss:  17.40539131164551\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  325\n",
            "Train loss:  4.304624385303921\n",
            "Test loss:  19.583810806274414\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  326\n",
            "Train loss:  4.170551637808482\n",
            "Test loss:  18.64036636352539\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  327\n",
            "Train loss:  3.7373310128847756\n",
            "Test loss:  16.901589584350585\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  328\n",
            "Train loss:  4.579859209060669\n",
            "Test loss:  21.919141960144042\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  329\n",
            "Train loss:  3.85041246149275\n",
            "Test loss:  22.02104072570801\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  330\n",
            "Train loss:  4.030079261461894\n",
            "Test loss:  18.157105255126954\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  331\n",
            "Train loss:  4.024673088391622\n",
            "Test loss:  18.603408432006837\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  332\n",
            "Train loss:  4.356257959206899\n",
            "Test loss:  18.26625289916992\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  333\n",
            "Train loss:  4.382486006948683\n",
            "Test loss:  20.706344604492188\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  334\n",
            "Train loss:  4.261938815646701\n",
            "Test loss:  16.74980812072754\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  335\n",
            "Train loss:  4.709694014655219\n",
            "Test loss:  22.81106605529785\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  336\n",
            "Train loss:  4.260360421074761\n",
            "Test loss:  18.30653533935547\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  337\n",
            "Train loss:  3.7012251959906686\n",
            "Test loss:  23.458019065856934\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  338\n",
            "Train loss:  3.351252273718516\n",
            "Test loss:  18.47382011413574\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  339\n",
            "Train loss:  5.325755374961429\n",
            "Test loss:  21.587827301025392\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  340\n",
            "Train loss:  3.8634044143888686\n",
            "Test loss:  11.684086227416993\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  341\n",
            "Train loss:  7.160585912068685\n",
            "Test loss:  16.853792190551758\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  342\n",
            "Train loss:  5.1671254767311945\n",
            "Test loss:  18.402996826171876\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  343\n",
            "Train loss:  4.727283600966135\n",
            "Test loss:  20.941661071777343\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  344\n",
            "Train loss:  4.48217346933153\n",
            "Test loss:  19.54192657470703\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  345\n",
            "Train loss:  4.159251782629225\n",
            "Test loss:  20.692719650268554\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  346\n",
            "Train loss:  3.8205319232410853\n",
            "Test loss:  23.18637046813965\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  347\n",
            "Train loss:  4.018722644117143\n",
            "Test loss:  18.64875183105469\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  348\n",
            "Train loss:  4.157769771416982\n",
            "Test loss:  24.183946228027345\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  349\n",
            "Train loss:  3.482348112265269\n",
            "Test loss:  21.2113525390625\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  350\n",
            "Train loss:  4.230646071169112\n",
            "Test loss:  20.909225082397462\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  351\n",
            "Train loss:  4.470519144005245\n",
            "Test loss:  21.177815246582032\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  352\n",
            "Train loss:  4.104952139324612\n",
            "Test loss:  21.199243545532227\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  353\n",
            "Train loss:  3.9717256585756937\n",
            "Test loss:  16.296495819091795\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  354\n",
            "Train loss:  4.139986741542816\n",
            "Test loss:  22.440268325805665\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  355\n",
            "Train loss:  3.842836014429728\n",
            "Test loss:  19.886962127685546\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  356\n",
            "Train loss:  4.720696428087023\n",
            "Test loss:  19.189683532714845\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  357\n",
            "Train loss:  3.607762118180593\n",
            "Test loss:  18.87317657470703\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  358\n",
            "Train loss:  4.11034206284417\n",
            "Test loss:  18.519851684570312\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  359\n",
            "Train loss:  3.676182230313619\n",
            "Test loss:  22.341061973571776\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  360\n",
            "Train loss:  3.594986875851949\n",
            "Test loss:  17.429639434814455\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  361\n",
            "Train loss:  3.674203015698327\n",
            "Test loss:  25.464805793762206\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  362\n",
            "Train loss:  3.9375064743889703\n",
            "Test loss:  20.24305992126465\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  363\n",
            "Train loss:  3.6225851390096877\n",
            "Test loss:  16.479225158691406\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  364\n",
            "Train loss:  4.438351119889154\n",
            "Test loss:  20.676298522949217\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  365\n",
            "Train loss:  4.0902871198124355\n",
            "Test loss:  19.39526023864746\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  366\n",
            "Train loss:  3.9342948105600146\n",
            "Test loss:  24.423244857788085\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  367\n",
            "Train loss:  2.580427751276228\n",
            "Test loss:  17.985209655761718\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  368\n",
            "Train loss:  3.887052673763699\n",
            "Test loss:  23.748402976989745\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  369\n",
            "Train loss:  3.3577006234063043\n",
            "Test loss:  20.181536865234374\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  370\n",
            "Train loss:  4.157814162307315\n",
            "Test loss:  20.234898376464844\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  371\n",
            "Train loss:  3.495403916305966\n",
            "Test loss:  22.00208969116211\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  372\n",
            "Train loss:  4.082166684998406\n",
            "Test loss:  14.841733932495117\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  373\n",
            "Train loss:  4.384235538376702\n",
            "Test loss:  20.136134719848634\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  374\n",
            "Train loss:  3.753733135594262\n",
            "Test loss:  21.743860244750977\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  375\n",
            "Train loss:  3.5016261802779303\n",
            "Test loss:  19.854512786865236\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  376\n",
            "Train loss:  3.5995022654533386\n",
            "Test loss:  25.645717239379884\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  377\n",
            "Train loss:  3.1073068976402283\n",
            "Test loss:  21.066061782836915\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  378\n",
            "Train loss:  3.6999636080529954\n",
            "Test loss:  14.33370475769043\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  379\n",
            "Train loss:  4.1024741874800785\n",
            "Test loss:  19.3257194519043\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  380\n",
            "Train loss:  3.7963164687156676\n",
            "Test loss:  23.127711868286134\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  381\n",
            "Train loss:  4.0053720315297445\n",
            "Test loss:  19.259745788574218\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  382\n",
            "Train loss:  3.552909196747674\n",
            "Test loss:  22.164965629577637\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  383\n",
            "Train loss:  2.851407335864173\n",
            "Test loss:  14.664644241333008\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  384\n",
            "Train loss:  5.462685117456648\n",
            "Test loss:  20.69175567626953\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  385\n",
            "Train loss:  4.225813558366563\n",
            "Test loss:  22.003216552734376\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  386\n",
            "Train loss:  3.28134968413247\n",
            "Test loss:  14.390481948852539\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  387\n",
            "Train loss:  5.078866570525699\n",
            "Test loss:  21.20246467590332\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  388\n",
            "Train loss:  4.1907028900252445\n",
            "Test loss:  22.062042045593262\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  389\n",
            "Train loss:  3.628217295805613\n",
            "Test loss:  24.123012924194335\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  390\n",
            "Train loss:  3.361568564838833\n",
            "Test loss:  20.52577152252197\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  391\n",
            "Train loss:  2.6865280681186254\n",
            "Test loss:  26.57780532836914\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  392\n",
            "Train loss:  3.059726970725589\n",
            "Test loss:  16.48475227355957\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  393\n",
            "Train loss:  3.822921273443434\n",
            "Test loss:  22.45836982727051\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  394\n",
            "Train loss:  3.07348745531506\n",
            "Test loss:  14.156877517700195\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  395\n",
            "Train loss:  5.322916804419624\n",
            "Test loss:  20.81412239074707\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  396\n",
            "Train loss:  3.349972490469615\n",
            "Test loss:  23.345775985717772\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  397\n",
            "Train loss:  3.8722038361761304\n",
            "Test loss:  18.665607070922853\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  398\n",
            "Train loss:  4.036481236086951\n",
            "Test loss:  20.81996593475342\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  399\n",
            "Train loss:  2.9554154833157855\n",
            "Test loss:  20.141903686523438\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  400\n",
            "Train loss:  3.4143107308281793\n",
            "Test loss:  21.979533767700197\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  401\n",
            "Train loss:  5.092478111055162\n",
            "Test loss:  17.35182876586914\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  402\n",
            "Train loss:  4.204463843504588\n",
            "Test loss:  21.19515781402588\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  403\n",
            "Train loss:  3.4138930016093783\n",
            "Test loss:  22.35669860839844\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  404\n",
            "Train loss:  3.3987703151173063\n",
            "Test loss:  25.01736946105957\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  405\n",
            "Train loss:  2.8543155630429586\n",
            "Test loss:  18.087064361572267\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  406\n",
            "Train loss:  3.6611899190478856\n",
            "Test loss:  19.682866287231445\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  407\n",
            "Train loss:  3.653394377231598\n",
            "Test loss:  19.265338706970216\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  408\n",
            "Train loss:  5.006792460547553\n",
            "Test loss:  18.877584075927736\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  409\n",
            "Train loss:  3.2881338238716125\n",
            "Test loss:  25.050103759765626\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  410\n",
            "Train loss:  3.3322328209877012\n",
            "Test loss:  18.40054244995117\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  411\n",
            "Train loss:  4.519677392641703\n",
            "Test loss:  23.71636962890625\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  412\n",
            "Train loss:  2.7197094162305198\n",
            "Test loss:  25.554682731628418\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  413\n",
            "Train loss:  2.9726079874568514\n",
            "Test loss:  18.94817695617676\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  414\n",
            "Train loss:  4.092935775385962\n",
            "Test loss:  23.004365539550783\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  415\n",
            "Train loss:  2.5743580910894606\n",
            "Test loss:  17.987540435791015\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  416\n",
            "Train loss:  3.3910579575432673\n",
            "Test loss:  22.63813171386719\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  417\n",
            "Train loss:  2.4996169845263165\n",
            "Test loss:  22.603990745544433\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  418\n",
            "Train loss:  2.6994514558050366\n",
            "Test loss:  22.456641387939452\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  419\n",
            "Train loss:  3.729468482070499\n",
            "Test loss:  20.561417388916016\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  420\n",
            "Train loss:  3.271491809686025\n",
            "Test loss:  19.553074645996094\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  421\n",
            "Train loss:  3.502443379826016\n",
            "Test loss:  20.70224208831787\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  422\n",
            "Train loss:  2.6776244044303894\n",
            "Test loss:  24.454515266418458\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  423\n",
            "Train loss:  2.7059672991434733\n",
            "Test loss:  21.236875915527342\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  424\n",
            "Train loss:  3.1323113335503474\n",
            "Test loss:  20.383251571655272\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  425\n",
            "Train loss:  3.140825883547465\n",
            "Test loss:  18.77568664550781\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  426\n",
            "Train loss:  3.57809032201767\n",
            "Test loss:  21.679369926452637\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  427\n",
            "Train loss:  2.7886186520258587\n",
            "Test loss:  22.355813789367676\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  428\n",
            "Train loss:  2.8304368721114264\n",
            "Test loss:  18.89365882873535\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  429\n",
            "Train loss:  4.429069488578373\n",
            "Test loss:  19.834275436401366\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  430\n",
            "Train loss:  2.6374261657396953\n",
            "Test loss:  22.056446075439453\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  431\n",
            "Train loss:  3.053539876143138\n",
            "Test loss:  22.588573837280272\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  432\n",
            "Train loss:  2.585507133271959\n",
            "Test loss:  26.11585350036621\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  433\n",
            "Train loss:  3.078799545764923\n",
            "Test loss:  21.090392303466796\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  434\n",
            "Train loss:  4.166880853970846\n",
            "Test loss:  27.102742767333986\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  435\n",
            "Train loss:  5.141812368233999\n",
            "Test loss:  30.788168716430665\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  436\n",
            "Train loss:  4.408265749613444\n",
            "Test loss:  21.558798599243165\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  437\n",
            "Train loss:  3.9400333656205073\n",
            "Test loss:  21.437360763549805\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  438\n",
            "Train loss:  3.839316689968109\n",
            "Test loss:  19.98937072753906\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  439\n",
            "Train loss:  3.658642674816979\n",
            "Test loss:  21.67088508605957\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  440\n",
            "Train loss:  3.375156542989943\n",
            "Test loss:  16.78177490234375\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  441\n",
            "Train loss:  3.760361871454451\n",
            "Test loss:  23.065810012817384\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  442\n",
            "Train loss:  3.258675277233124\n",
            "Test loss:  17.01494140625\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  443\n",
            "Train loss:  3.938712557156881\n",
            "Test loss:  20.519993209838866\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  444\n",
            "Train loss:  3.154058003425598\n",
            "Test loss:  26.310174560546876\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  445\n",
            "Train loss:  2.5303820504082575\n",
            "Test loss:  25.56704978942871\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  446\n",
            "Train loss:  3.0412690507041082\n",
            "Test loss:  24.362520027160645\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  447\n",
            "Train loss:  2.6777235865592957\n",
            "Test loss:  17.54319038391113\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  448\n",
            "Train loss:  3.860382017824385\n",
            "Test loss:  21.192983055114745\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  449\n",
            "Train loss:  2.681704534424676\n",
            "Test loss:  21.304851913452147\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  450\n",
            "Train loss:  3.6995443754725987\n",
            "Test loss:  23.777450180053712\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  451\n",
            "Train loss:  4.331468988789452\n",
            "Test loss:  19.80100440979004\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  452\n",
            "Train loss:  3.0339165727297464\n",
            "Test loss:  25.66277084350586\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  453\n",
            "Train loss:  2.8591602126757305\n",
            "Test loss:  20.6748046875\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  454\n",
            "Train loss:  2.5210480252901712\n",
            "Test loss:  28.14940490722656\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  455\n",
            "Train loss:  2.406114297442966\n",
            "Test loss:  25.659335708618165\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  456\n",
            "Train loss:  1.5546401725875008\n",
            "Test loss:  21.40254192352295\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  457\n",
            "Train loss:  2.846232087082333\n",
            "Test loss:  21.914712715148926\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  458\n",
            "Train loss:  3.1968338012695314\n",
            "Test loss:  20.089252471923828\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  459\n",
            "Train loss:  3.528267240524292\n",
            "Test loss:  18.791123962402345\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  460\n",
            "Train loss:  2.6535344349013434\n",
            "Test loss:  22.258939933776855\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  461\n",
            "Train loss:  2.837452064620124\n",
            "Test loss:  18.634631729125978\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  462\n",
            "Train loss:  3.62265143526925\n",
            "Test loss:  23.133279418945314\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  463\n",
            "Train loss:  3.475588211748335\n",
            "Test loss:  21.776939392089844\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  464\n",
            "Train loss:  3.183961786164178\n",
            "Test loss:  21.027596282958985\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  465\n",
            "Train loss:  3.3891780575116477\n",
            "Test loss:  23.421513938903807\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  466\n",
            "Train loss:  2.754792849222819\n",
            "Test loss:  19.25545883178711\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  467\n",
            "Train loss:  2.7337535368071664\n",
            "Test loss:  23.64769458770752\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  468\n",
            "Train loss:  2.5018872804111902\n",
            "Test loss:  22.169699478149415\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  469\n",
            "Train loss:  2.8270187099774677\n",
            "Test loss:  16.45890235900879\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  470\n",
            "Train loss:  4.014722473091549\n",
            "Test loss:  25.54550971984863\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  471\n",
            "Train loss:  2.728055206934611\n",
            "Test loss:  19.854396057128906\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  472\n",
            "Train loss:  2.823732872804006\n",
            "Test loss:  24.8144229888916\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  473\n",
            "Train loss:  2.74637328253852\n",
            "Test loss:  17.904501342773436\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  474\n",
            "Train loss:  3.1958812435468036\n",
            "Test loss:  20.80952606201172\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  475\n",
            "Train loss:  4.136749821239047\n",
            "Test loss:  24.74060974121094\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  476\n",
            "Train loss:  3.3533909413549634\n",
            "Test loss:  16.55247688293457\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  477\n",
            "Train loss:  4.713326011763678\n",
            "Test loss:  18.353285598754884\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  478\n",
            "Train loss:  3.675228696399265\n",
            "Test loss:  20.773757934570312\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  479\n",
            "Train loss:  3.184197598033481\n",
            "Test loss:  22.499617195129396\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  480\n",
            "Train loss:  2.423280492093828\n",
            "Test loss:  22.131625366210937\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  481\n",
            "Train loss:  2.16077524556054\n",
            "Test loss:  28.463565063476562\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  482\n",
            "Train loss:  3.474072636498345\n",
            "Test loss:  22.458727264404295\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  483\n",
            "Train loss:  3.88841223054462\n",
            "Test loss:  23.736062240600585\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  484\n",
            "Train loss:  2.6481819987297057\n",
            "Test loss:  23.58851318359375\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  485\n",
            "Train loss:  2.360166434446971\n",
            "Test loss:  19.350613403320313\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  486\n",
            "Train loss:  2.586579586399926\n",
            "Test loss:  22.581768798828126\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  487\n",
            "Train loss:  2.8800372772746616\n",
            "Test loss:  25.311521530151367\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  488\n",
            "Train loss:  3.155971648957994\n",
            "Test loss:  20.68580322265625\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  489\n",
            "Train loss:  2.5313964817259045\n",
            "Test loss:  21.744777107238768\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  490\n",
            "Train loss:  2.3006717125574747\n",
            "Test loss:  21.89932975769043\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  491\n",
            "Train loss:  2.268740157286326\n",
            "Test loss:  19.34456100463867\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  492\n",
            "Train loss:  2.804928551779853\n",
            "Test loss:  25.097705841064453\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  493\n",
            "Train loss:  3.7401675740877787\n",
            "Test loss:  16.033019256591796\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  494\n",
            "Train loss:  4.581850527392493\n",
            "Test loss:  20.59485969543457\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  495\n",
            "Train loss:  3.5317185441652934\n",
            "Test loss:  23.70002841949463\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  496\n",
            "Train loss:  2.4347692184978063\n",
            "Test loss:  22.87589454650879\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  497\n",
            "Train loss:  2.389692696597841\n",
            "Test loss:  24.757950210571288\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  498\n",
            "Train loss:  3.075708360142178\n",
            "Test loss:  20.749971008300783\n",
            "----------------------------------------------------------------\n",
            "Epoch number:  499\n",
            "Train loss:  3.1794907053311667\n",
            "Test loss:  24.816479110717772\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}